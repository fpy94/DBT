{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from scipy import stats\n",
    "\n",
    "class ACME(nn.Module):\n",
    "    #reimplementation of ACME without attention module\n",
    "    def __init__(self):\n",
    "        super(ACME,self).__init__()\n",
    "\n",
    "        self.pconv1=nn.Sequential(\n",
    "            nn.Conv1d(20,128,3,padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mconv1=nn.Sequential(\n",
    "            nn.Conv1d(20,128,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.mconv2=nn.Sequential(\n",
    "            nn.Conv1d(128,128,3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.dense0=nn.Sequential(\n",
    "            nn.Linear(3752,256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense1=nn.Sequential(\n",
    "            nn.Linear(5248,256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense2=nn.Sequential(\n",
    "            nn.Linear(4096,256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense3=nn.Sequential(\n",
    "            nn.Linear(768,64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense4=nn.Sequential(\n",
    "            nn.Linear(64,1)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "        self.loss_fn=nn.MSELoss()\n",
    "\n",
    "    def extract_feature(self,sx,mx):\n",
    "        sx=Variable(sx.cuda())\n",
    "        mx=Variable(mx.cuda())\n",
    "#         sx=Variable(sx)\n",
    "#         mx=Variable(mx)\n",
    "        sx=sx.permute(0,2,1)\n",
    "        mx=mx.permute(0,2,1)\n",
    "        pep_conv=self.pconv1(sx)#batch,128,24\n",
    "        mhc_conv_1=self.mconv1(mx)#batch,128,17\n",
    "        mhc_conv_2=self.mconv2(mhc_conv_1)#batch,128,8\n",
    "        flat_pep_0=pep_conv.view(pep_conv.size(0),-1)#3072\n",
    "        flat_pep_1=pep_conv.view(pep_conv.size(0),-1)#3072\n",
    "        flat_pep_2=pep_conv.view(pep_conv.size(0),-1)#3072\n",
    "        flat_mhc_0=mx.contiguous().view(mx.size(0),-1)#680\n",
    "        flat_mhc_1=mhc_conv_1.view(mhc_conv_1.size(0),-1)#2176\n",
    "        flat_mhc_2=mhc_conv_2.view(mhc_conv_2.size(0),-1)#1024\n",
    "        cat_0=torch.cat((flat_pep_0, flat_mhc_0),1)#3752\n",
    "        cat_1=torch.cat((flat_pep_1, flat_mhc_1),1)#5248\n",
    "        cat_2=torch.cat((flat_pep_2, flat_mhc_2),1)#4096\n",
    "        fc1_0=self.dense0(cat_0)#256\n",
    "        fc1_1=self.dense1(cat_1)\n",
    "        fc1_2=self.dense2(cat_2)\n",
    "        merge_1=torch.cat((fc1_0, fc1_1,fc1_2),1)#768\n",
    "        self.feature=self.dense3(merge_1)#64\n",
    "        return self.feature\n",
    "\n",
    "    def forward(self,sx,mx):\n",
    "        self.feature=self.extract_feature(sx,mx)\n",
    "        self.represent=self.dense4(self.feature)\n",
    "        out=self.sigmoid(self.represent)\n",
    "        return out\n",
    "\n",
    "    def train_loop(self,epoch,train_loader,optimizer):\n",
    "        avg_loss=0\n",
    "        avg_l2loss=0\n",
    "        SCORE=[]\n",
    "        LABEL=[]\n",
    "\n",
    "        for batch_idx,(sx,mx,y) in enumerate(train_loader):\n",
    "            scores=self.forward(sx,mx)\n",
    "            y=Variable(y.cuda())\n",
    "            loss=self.loss_fn(scores,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss+=loss.item()\n",
    "            SCORE.append(scores.cpu().data.numpy())\n",
    "            LABEL.append(y.cpu().data.numpy())\n",
    "        S=np.concatenate(SCORE,0)\n",
    "        L=np.concatenate(LABEL,0)\n",
    "        pr,_=stats.pearsonr(S[:,0],L[:,0])\n",
    "        avg_loss/=batch_idx\n",
    "        print('Epoch {:d} | Loss {:f}| pr {}'.format(epoch,avg_loss,pr))\n",
    "\n",
    "\n",
    "    def test_loop(self,epochs,test_loader):\n",
    "        avg_loss=0\n",
    "        avg_l2loss=0\n",
    "        SCORE=[]\n",
    "        LABEL=[]\n",
    "\n",
    "        for batch_idx,(sx,mx,y) in enumerate(test_loader):\n",
    "            scores=self.forward(sx,mx)\n",
    "            y=Variable(y.cuda())\n",
    "            l2_loss=torch.norm(self.parms,2)\n",
    "            loss=self.loss_fn(scores,y)+self.lambda_*l2_loss\n",
    "            avg_loss+=loss.item()\n",
    "            avg_l2loss+=l2_loss.item()\n",
    "            SCORE.append(scores.cpu().data.numpy())\n",
    "            LABEL.append(y.cpu().data.numpy())\n",
    "        S=np.concatenate(SCORE,0)\n",
    "        L=np.concatenate(LABEL,0)\n",
    "        pr,_=stats.pearsonr(S[:,0],L[:,0])\n",
    "        avg_loss/=batch_idx\n",
    "        avg_l2loss/=batch_idx\n",
    "        return avg_loss,pr\n",
    "    def Test(self,test_loader):\n",
    "        tY=[]\n",
    "        tScore=[]\n",
    "        for batch_idx,(sx,mx,y) in enumerate(test_loader):\n",
    "            scores=self.forward(sx,mx)\n",
    "            tY.append(y)\n",
    "            tScore.append(scores.clone().detach())\n",
    "        tY=torch.cat(tY,0)\n",
    "        tScore=torch.cat(tScore,0)\n",
    "        return tScore.cpu().data.numpy(),tY.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "import random\n",
    "from scipy import stats\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "from dataset import *\n",
    "from sys import argv\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset\n",
    "from early_stopping import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_process/cvdata.pkl','rb') as f:\n",
    "    data=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "MAX_EPOCHS=20\n",
    "bs=64\n",
    "lr=1e-3\n",
    "savepath='./pretrain_model'\n",
    "if not os.path.exists(savepath):\n",
    "    os.makedirs(savepath)\n",
    "models=[[] for _ in range(5)]\n",
    "for i in range(4):\n",
    "    for cv in range(5):\n",
    "        train_data,test_data=data[cv]\n",
    "        random.shuffle(train_data)\n",
    "        valid_data=train_data[:len(train_data)//10]\n",
    "        traindata=train_data[len(train_data)//10:]\n",
    "        train_loader=DataLoader(allele_dataset(traindata),batch_size=bs,shuffle=True)\n",
    "        valid_loader=DataLoader(allele_dataset(valid_data),batch_size=bs,shuffle=True)\n",
    "        test_loader=DataLoader(allele_dataset(test_data),batch_size=bs,shuffle=False)\n",
    "        net=ACME()\n",
    "        net=net.cuda()\n",
    "        optimizer=torch.optim.Adam(net.parameters(),lr=lr)\n",
    "        for epochs in range(MAX_EPOCHS):\n",
    "            net.train()\n",
    "            if epochs==0:\n",
    "                while True:\n",
    "                    net.train_loop(epochs,train_loader,optimizer)\n",
    "                    tests,testy=net.Test(valid_loader)\n",
    "                    pr,_=stats.pearsonr(tests[:,0],testy[:,0])\n",
    "                    print(pr)\n",
    "                    if pr>0.75:\n",
    "                        models[cv].append(net.state_dict())\n",
    "    #                     torch.save(net.state_dict(), os.path.join(savepath,'cv_'+str(cv)+'_e_0.pkl'))\n",
    "                        break\n",
    "                    else:\n",
    "                        net=ACME()\n",
    "                        net.cuda()\n",
    "            else:\n",
    "                break\n",
    "                net.train_loop(epochs,train_loader,optimizer)\n",
    "                tests,testy=net.Test(valid_loader)\n",
    "                pr,_=stats.pearsonr(tests[:,0],testy[:,0])\n",
    "                print(pr)\n",
    "            if pr>0.84:\n",
    "                models[cv].append(net.state_dict())\n",
    "    #                 torch.save(net.state_dict(), os.path.join(savepath,'cv_'+str(cv)+'.pkl'))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pretrain_model/e_0_rep.pkl','wb') as f:\n",
    "    pickle.dump(models,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
